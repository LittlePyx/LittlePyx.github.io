{"total":1,"pageSize":10,"pageCount":1,"data":[{"title":"神经网络","slug":"神经网络","date":"2022-08-13T12:32:56.000Z","updated":"2022-08-13T13:57:18.829Z","comments":true,"path":"api/articles/神经网络.json","excerpt":"","keywords":null,"cover":null,"content":"<h4 id=\"M-P神经元模型-阈值逻辑单元\"><a href=\"#M-P神经元模型-阈值逻辑单元\" class=\"headerlink\" title=\"M-P神经元模型(阈值逻辑单元)\"></a>M-P神经元模型(阈值逻辑单元)</h4><p>神经元接收到来自$n$个神经元传递过来的输入信号，通过带权重的连接进行传递，神经元接收到的总输入值将与神经元的阈值进行比较，然后通过“激活函数”处理以产生神经元的输出。</p>\n<p>理想的激活函数为阶跃函数，但阶跃函数不连续不光滑，故常用Sigmoid函数。<script type=\"math/tex\">sigmoid(x)=\\dfrac{1}{1+e^{-x}}</script></p>\n<h4 id=\"感知机\"><a href=\"#感知机\" class=\"headerlink\" title=\"感知机\"></a>感知机</h4><p>感知机由两层神经元组成，输入层接收外界输入信号后传递给输出层，输出层是M-P神经元模型。</p>\n<p>给定训练数据集，权重<script type=\"math/tex\">w_i</script>以及阈值 $\\theta$ 可通过学习得到，阈值$\\theta$可以看作一个固定输入为-1.0的哑节点所对应的权重 $w_{n+1}$，这样可统一为权重的学习。</p>\n<p><strong>学习规则：</strong>对训练样例 <script type=\"math/tex\">(\\mathbf{x},y)</script>，若当前感知机的输出为 $\\hat{y}$，则感知机的权重调整规则为：$w_i\\leftarrow w_i+\\Delta w_i$,$\\Delta w_i=\\eta (y-\\hat{y})x_i$。神经网络学到的东西，蕴含在连接权与阈值中。</p>\n<h3 id=\"BP算法-误差逆传播算法\"><a href=\"#BP算法-误差逆传播算法\" class=\"headerlink\" title=\"BP算法(误差逆传播算法)\"></a>BP算法(误差逆传播算法)</h3><p>对于一个拥有 $d$ 个输入神经元，$l$ 个输出神经元，$q$ 个隐层神经元的多层前馈网络。输出层第$j$个神经元的阈值为 <script type=\"math/tex\">\\theta_j</script>，隐层第$h$个神经网络的阈值用 <script type=\"math/tex\">\\gamma_h</script>，输入层第 $i$ 个神经元与隐层第 $h$ 个神经元之间的连接权为 <script type=\"math/tex\">v_{ih}</script>，隐层第 $h$ 个神经元与输出层第$j$个神经元之间的连接权为 <script type=\"math/tex\">w_{hj}</script> 记隐层第$h$个神经元接收到的输入为 <script type=\"math/tex\">\\alpha_h=\\sum_{h=1}^{d}v_{ih}x_i</script>，输出层第$j$个神经元接收到的输入为 </p>\n<script type=\"math/tex; mode=display\">\\displaystyle \\beta_j=\\sum_{h=1}^{q}w_{hj}b_h</script><p>其中<script type=\"math/tex\">b_h</script>为隐层第 <script type=\"math/tex\">h</script> 个神经元的输出。</p>\n<p>假设隐层和输出层神经元都用Sigmoid函数，对训练例 $(\\mathbf{x}_k,\\mathbf{y}_k)$，假定神经网络的输出为 </p>\n<script type=\"math/tex; mode=display\">\\hat{\\mathbf{y_k}}=(\\hat{y_1^k},\\hat{y_2^k},\\cdots,\\hat{y_l^k})</script><p>即 <script type=\"math/tex\">\\hat{y_j^k}=f(\\beta_j-\\theta_j)</script>,则均方误差为</p>\n<script type=\"math/tex; mode=display\">E_k=\\dfrac{1}{2}\\sum_{j=1}^{l}(\\hat{y_j^k}-y_j^k)^2</script><p>网络中有 $(d+l+1)q+l$ 个参数需确定：</p>\n<ul>\n<li><p>输入层到隐层的连接权值：$d\\times q$ 个</p>\n</li>\n<li><p>隐层到输出层的连接权值：$q\\times l$ 个</p>\n</li>\n<li><p>隐层神经元阈值：$q$ 个</p>\n</li>\n<li><p>输出层神经元阈值：$l$ 个</p>\n</li>\n</ul>\n<p>任意参数 $v$ 的更新：$v\\leftarrow v+\\Delta t$</p>\n<p>BP算法基于<strong>梯度下降</strong>策略，以目标的负梯度方向对参数调整，对于隐层到输出层的连接权 <script type=\"math/tex\">w_{jk}</script> 为例，有 </p>\n<script type=\"math/tex; mode=display\">\\Delta w_{hj}=-\\eta \\dfrac{\\partial E_k}{\\partial w_{hj}}=-\\eta\\dfrac{\\partial E_k}{\\partial \\hat{y_j^k}}\\cdot \\dfrac{\\partial \\hat{y_j^k}}{\\partial \\beta_j}\\cdot \\dfrac{\\partial \\beta_j}{\\partial w_{hj}}=\\eta\\hat{y_j^k}(1-\\hat{y_j^k})(y_j^k-\\hat{y_j^k})\\cdot b_h</script><p>其中令 <script type=\"math/tex\">g_{j}=-\\dfrac{\\partial E_k}{\\partial \\hat{y_j^k}}\\cdot \\dfrac{\\partial \\hat{y_j^k}}{\\partial \\beta_j}</script>，  得到 $\\Delta w_{hj}=\\eta g_j b_h$，相同，令 <script type=\"math/tex\">e_h=-\\dfrac{\\partial E_k}{\\partial b_h}\\dfrac{\\partial b_h}{\\partial \\alpha_h}</script></p>\n<p>类似可以得到 <script type=\"math/tex\">\\Delta \\theta_j=-\\eta g_j\\qquad\\Delta v_{ih}=\\eta e_hx_i\\qquad \\Delta \\gamma_h=-\\eta e_h</script></p>\n<p><script type=\"math/tex\">\\eta\\in(0,1)</script>学习率，控制算法每轮迭代中的更新步长。学习率太大容易振荡，太小收敛速度过慢</p>\n<p>算法具体步骤：</p>\n<ul>\n<li>在$(0,1)$范围内初始化所有的连接权和阈值</li>\n<li>根据当前的参数计算当前样本的输出 $\\hat{\\mathbf{y}_k}$</li>\n<li>分别计算出输出层的梯度项 $g_j$ 和隐层的梯度项 $e_h$</li>\n<li>更新连接权和阈值</li>\n<li>重复2-4直到达到停止条件</li>\n</ul>\n<p>BP算法的目标是最小化训练集上的累积误差 $\\displaystyle E=\\frac{1}{m}\\sum_{k=1}^{m}E_k$</p>\n<p><code>上述介绍的是标准BP算法</code></p>\n<ul>\n<li><p>标准BP算法：每次更新只针对单个样例，参数更新得非常频繁，而且对不同样例进行更新的效果可能出现抵消的现象</p>\n</li>\n<li><p>累积BP算法：直接针对累积误差最小化，它在读取整个训练集一遍后才对参数进行更新，其参数更新的频率低得多，但很多任务中，累积误差下降到一定程度后，进一步下降会非常缓慢，这时标准BP算法往往会更快获得较好的解，尤其是在训练集非常大时更明显</p>\n</li>\n</ul>\n<h4 id=\"缓解过拟合\"><a href=\"#缓解过拟合\" class=\"headerlink\" title=\"缓解过拟合\"></a>缓解过拟合</h4><p>过拟合：训练误差持续降低但测试误差却可能上升</p>\n<ul>\n<li><p>策略1：早停</p>\n<p>将数据分为训练集和测试集，训练集计算梯度、更新连接权和阈值，验证集用来估计误差，若训练集误差降低，验证集误差升高则停止训练。</p>\n</li>\n<li><p>策略2：正则化</p>\n<p>在误差目标函数中增加一个用来描述网络复杂度的部分，例如连接权与阈值的平方和，仍令 <script type=\"math/tex\">E_k</script> 表示第 $k$ 个训练样例上的误差，<script type=\"math/tex\">w_i</script> 表示连接权和阈值，则误差目标函数表示为<script type=\"math/tex\">E=\\lambda \\frac{1}{m}\\sum_{k=1}^{m}E_k+(1-\\lambda)\\sum_{i}w_i^2</script> ，其中 $\\lambda\\in(0,1)$ 用于对经验误差与网络复杂度这两项折中，常通过交叉验证法估计。</p>\n</li>\n</ul>\n<h4 id=\"防止陷入局部最优\"><a href=\"#防止陷入局部最优\" class=\"headerlink\" title=\"防止陷入局部最优\"></a>防止陷入局部最优</h4><p>梯度下降法搜索寻优容易陷入局部最小，常用以下方法跳出局部最小</p>\n<ol>\n<li>以多组不同的参数值初始化多个神经网络，按标准方法训练后对比所有局部最优解，得到最可能接近全局最优解的。</li>\n<li>使用<strong>模拟退火</strong>，在每一步都以一定概率接受比当前解更差的结果，有助于跳出局部极小，在每步迭代过程中，接受“次优解”的概率要随着时间的推移而逐渐降低，从而保证算法稳定。</li>\n<li>使用随机梯度下降，计算梯度时加入随机因素，即便陷入局部极小点，计算出的梯度仍可能不为0，有机会跳出局部极小继续搜索。</li>\n</ol>\n<hr>\n<h2 id=\"其他神经网络\"><a href=\"#其他神经网络\" class=\"headerlink\" title=\"其他神经网络\"></a>其他神经网络</h2><h4 id=\"RBF网络（径向基函数）\"><a href=\"#RBF网络（径向基函数）\" class=\"headerlink\" title=\"RBF网络（径向基函数）\"></a>RBF网络（径向基函数）</h4><p>是一种单隐层前馈神经网络，使用径向基函数作为隐层神经元激活函数，输出层是对隐层神经元输出的线性组合。假定输入为 $d$ 维向量 $\\mathbf{x}$ 。则RBF网络课表示为$\\varphi(\\mathbf{x})=\\sum_{i=1}^{q}w_i\\rho(\\mathbf{x},\\mathbf{c_i})$ 。 $c_i$ 和 $w_i$ 分别是第 $i$ 个隐层神经元所对应的中心和权重，$\\rho(\\mathbf{x},\\mathbf{c_i})$ 是径向基函数，定义为样本 $\\mathbf{x}$ 到数据中心 $\\mathbf{c_i}$ 之间的欧氏距离的\\单调函数。常用的高斯径向基函数形如 $\\rho(\\mathbf{x},\\mathbf{c_i})=e^{-\\beta_i||\\mathbf{x}-\\mathbf{c_i}||^2}$</p>\n<p><strong>步骤</strong>：</p>\n<ol>\n<li><p>确定神经元中心 $\\mathbf{c_i}$ ，常用的方法有随机采样、聚类等</p>\n</li>\n<li><p>利用BP算法等确定参数 $w_i$ 和 $\\beta_i$ </p>\n</li>\n</ol>\n<h4 id=\"ART-自适应谐振理论-网络\"><a href=\"#ART-自适应谐振理论-网络\" class=\"headerlink\" title=\"ART(自适应谐振理论)网络\"></a>ART(自适应谐振理论)网络</h4><p>属于竞争型学习，由比较层、识别层、识别阈值、重置模块构成，比较层接受输入样本，传递给识别层神经元，识别层每个神经元对应一个模式类，<strong>神经元数目可在训练过程中动态增长</strong>以增加新的模式类。</p>\n<p>在接收到比较层的输入信号后，识别层神经元之间相互竞争以产生获胜神经元，竞争规则为计算输入向量与每个识别层神经元所对应的模式类的代表向量之间的距离，距离最小者胜。</p>\n<p>获胜的神经元向其他识别层神经元发送信号，抑制其激活。</p>\n<p>若输入向量与获胜神经元所对应的代表向量之间的相似度大于识别阈值，则当前输入样本将被归为该代表向量所属类别。</p>\n<p>同时网络的连接权将会更新，使以后在接收到相似输入样本时该模式类会计算出更大的相似度，从而使该获胜神经元有更到可能获胜。</p>\n<p>若相似度不大于识别阈值，则重置模块将在识别层增设一个新的神经元，其代表向量就设置为当前输入向量。</p>\n<p><code>当识别阈值较高时，输入样本将会被分成比较多、比较精细的模式类，而如果识别阈值较低，则会产生比较少、比较粗略的模式类。</code></p>\n<p><strong>特点</strong>：可进行增量学习或在线学习</p>\n<h4 id=\"SOM-自组织映射-网络\"><a href=\"#SOM-自组织映射-网络\" class=\"headerlink\" title=\"SOM(自组织映射)网络\"></a>SOM(自组织映射)网络</h4><p>是一种竞争学习的无监督神经网络，能将高维输入数据映射到低维空间，同时保持输入数据在高维空间的拓扑结构，即将高维空间中相似的样本映射到输出层中的邻近神经元。</p>\n<p>训练过程：</p>\n<ol>\n<li><p>接收到一个样本之后，每个输出层神经元计算该样本与自身携带的权向量之间的距离，距离最近的成为竞争获胜者，称为最佳匹配单元。</p>\n</li>\n<li><p>最佳匹配单元及其邻近神经元的权向量将被调整，使这些权向量与当前输入样本的距离缩小。</p>\n</li>\n<li><p>不断迭代直至收敛。</p>\n</li>\n</ol>\n<h4 id=\"级联相关网络\"><a href=\"#级联相关网络\" class=\"headerlink\" title=\"级联相关网络\"></a>级联相关网络</h4><p>是结构自适应网络，将网络的结构也作为学习的目标之一。</p>\n<p>在开始训练时只有输入层和输出层，处于最小拓扑结构。新的隐层神经元加入时，输入端连接权值是冻结固定的。相关是指最大化新神经元的输出与网络误差之间的相关性来训练相关的参数。</p>\n<p><strong>特点</strong>：无需设置网络层数，隐层神经元数目，且训练速度较快，但在数据较小时容易陷入过拟合。</p>\n<h4 id=\"Elman网络-递归神经网络\"><a href=\"#Elman网络-递归神经网络\" class=\"headerlink\" title=\"Elman网络(递归神经网络)\"></a>Elman网络(递归神经网络)</h4><p>允许网络中出现环状结构，可让一些神经元的输出反馈回来作为输入信号，从而处理与时间有关的动态变化。</p>\n<h4 id=\"Boltzmann机\"><a href=\"#Boltzmann机\" class=\"headerlink\" title=\"Boltzmann机\"></a>Boltzmann机</h4><p>为网络状态定义“能量”，能量最小化时网络达到理想状态。不说了</p>\n<h4 id=\"深度学习\"><a href=\"#深度学习\" class=\"headerlink\" title=\"深度学习\"></a>深度学习</h4><p>不说了</p>\n","text":"M-P神经元模型(阈值逻辑单元)神经元接收到来自$n$个神经元传递过来的输入信号，通过带权重的连接进行传递，神经元接收到的总输入值将与神经元的阈值进行比较，然后通过“激活函数”处理以产生神经元的输出。理想的激活函数为阶跃函数，但阶跃函数不连续不光滑，故常用Sigmoid函数。si","link":"","raw":null,"photos":[],"categories":[],"tags":[]}]}